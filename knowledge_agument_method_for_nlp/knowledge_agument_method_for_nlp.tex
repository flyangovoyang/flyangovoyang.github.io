\documentclass[11pt]{article}
\usepackage{xeCJK}
\usepackage[colorlinks,linkcolor=green, anchorcolor=green, citecolor=green]{hyperref}
%\usepackage{graphicx}

\begin{document}
\title{将知识融入NLP模型的方法总结}
\maketitle

\begin{abstract}
在大规模预训练模型出现之后，NLP开始越来越频繁地谈论“知识”。知识是模型获取常识、逻辑和其他外部信息的关键。文本对近几年SOTA工作将知识应用于NLU、NLG和常识推理的方法进行整理和总结。
\end{abstract}

\section{知识的定义}
近年来，通过结合更大的模型、更好的训练策略和更多的数据，NLP的发展得到了极大的推动，如BERT、RoBERTa和GPT。这些预训练模型可以有效地从文本中学到语言模式并生成高质量的、上下文感知的表征。然而，这些模型的输入只有原文本，因而很难获取到和概念、关系、常识相关的外部世界知识。本文用“知识”表示对模型预测目标输出很重要但是在当前模型输入中缺失的外部信息。知识应该被融入到模型的训练和推理中，因为知识对语言表征非常重要，它是实现高阶智能不可或缺的组成部分；其次，仅仅在输入文本上进行统计学习是无法获取到知识的。

\section{知识增强的NLU}

NLU任务的目的是在输入文本的基础上对词、短语、句子、篇章的属性做预测，比如情绪分析、命名实体识别和文本推理。从知识资源的维度可将知识增强NLU大致分为两类，一类是结构化知识（如知识图谱），另一类是非结构化知识（如文本语料）。

将结构化知识融入到NLU的工作又可分为两种，一种是基于概念或者实体嵌入的显式方法~\cite{zhang2019ernie,peters2019knowledge,liu2020k,yu2020identifying,zeng2020tri}，一种是通过实体遮掩预测的隐式方法~\cite{sun2019ernie,shen2020exploiting,xiong2020pretrained,wang2021kepler}。
比如ERNIE\cite{zhang2019ernie}使用TransE在知识图谱上显式地预训练实体嵌入，而EAE\cite{fevry2020entities}将其作为模型参数来学习。KEPLER\cite{wang2021kepler}基于描述文本使用预训练模型来隐式地计算实体嵌入。
最近，一些工作提出联合训练知识图谱模块和语言模型。比如JAKET\cite{yu2022jaket}提出使用知识模块来生成文本中实体的嵌入，而用语言模型来生成知识图谱中实体和关系的上下文感知的初始嵌入。Yu等人\cite{yu2022dict}和Xu等人\cite{xu2021fusing}提出使用词典描述作为额外的知识源 来做NLU和常识推理任务。

将非结构化知识融入NLU模型，一般需要一个文本检索模块来从知识语料中获取相关文本。使用非结构化知识有很多方法，尤其是对于开放领域的问答任务。比如Lee第一个通过ICT(inverse cloze task)来训练retriever，然后联合训练retriever和reader用于开放领域的问答；DPR通过监督学习训练retriever在开放领域问答上取得了更好的成绩；REALM预测遮掩的包含重要实体的span来联合预训练reader和retriever；KG-FiD提出在检索阶段通过检索文章之间的结构化关系来过滤噪声文章。

\section{知识增强的NLG}

NLG的目标是从各种形式的语言或非语言数据（如文本数据、图像数据和结构化知识图谱）中生成人类语言的可理解文本。和NLU方法不同的是，NLG方法一般都采用encoder-decoder框架，在生成过程中，如果在解码下一个token时引入知识会面临很多挑战。当前将知识融入NLG模型的方法大概分为三种：

\begin{itemize}
    \item 通过模型结构融入知识：知识关联的注意力机制，知识关联的拷贝/指针机制；
    \item 通过训练框架融入知识：posterior regularization, constraint-driven learning, semantic loss, knowledge-informed weak supervision；
    \item 通过推理方法融入知识，在推理时增加不同的知识约束来指导解码，比如lexical constraints, task-specific objectives, global inter-dependency；
\end{itemize}

如果从知识源的维度划分，结构化的知识融入方法可以划分以下四种：

\begin{itemize}
    \item 将预先计算的知识嵌入注入语言生成；
    \item 通过三元组信息将知识迁移到语言模型；
    \item 通过路径寻找策略来实现知识图谱推理；
    \item 用图神经网络来提高图嵌入。
\end{itemize}

非结构化的知识融入方法可以划分为以下两种：

\begin{itemize}
    \item 用检索信息来指导生成；
    \item 将背景知识建模到文本生成中。
\end{itemize}

\section{NLP常识和推理}

略。

\section{文章列表}

\begin{itemize}
    \item 知识增强NLU：\cite{zhang2019ernie},\cite{peters2019knowledge},\cite{liu2020k},\cite{ding2019cognitive},\cite{lv2020graph},\cite{yu2022jaket}
    \item 知识增强NLG：\cite{zhou2018commonsense},\cite{zhang2020grounded},\cite{ji2020language},\cite{lewis2020retrieval},\cite{wang2021retrieval}
    \item 常识和推理：\cite{ma2019towards},\cite{fan2020enhanced},\cite{liu2021kg},\cite{wang2021retrieval},\cite{guan2019story},\cite{guan2020knowledge}
    \item 相关综述：\cite{yu2020identifying},\cite{yang2021survey},\cite{zhang2022survey},\cite{wei2021knowledge}
\end{itemize}

\section{代表人物}

\begin{itemize}
    \item Chenguang Zhu
    \item Yichong Xu
    \item Xiang Ren
    \item Bill Yuchen Lin
    \item Meng Jiang
    \item Wenhao Yu
\end{itemize}

% bibtex方法
% 在bib文件中写清楚每个文献的内容
% 在正文使用如下两行代码
% style的取值有plain, 
\bibliographystyle{unsrt}
\bibliography{knowledge_agument_method_for_nlp}

\end{document}